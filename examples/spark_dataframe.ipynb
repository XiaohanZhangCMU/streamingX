{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "992ea5bf-f420-44e3-9c56-ab04c086bf05",
                    "showTitle": false,
                    "title": ""
                },
                "id": "vticii3WR_HN"
            },
            "source": [
                "# SPARK DataFrame\n",
                "In this tutorial, we will demonstrate how to use the streaming spark-converter to convert a spark dataframe to create a StreamingDataset. The users have the option to pass in a preprocessing job such as a tokenizer to the converter, which can be useful if materializing the intermediate dataframe is time consuming or taking extra development."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "ed551b02-c3a7-47dc-8aa2-8e6e501cf5e3",
                    "showTitle": false,
                    "title": ""
                },
                "id": "EV5xY06KR_HO"
            },
            "source": [
                "# Tutorial Covers\n",
                "1. Installation of libraries\n",
                "2. [Basic use-case] Convert spark dataframe to MDS format\n",
                "3. [Advanced use-case] convert spark dataframe into tokenized format and convert to MDS format."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "93f83c00-600f-4605-972c-5f0eb4a4152d",
                    "showTitle": false,
                    "title": ""
                },
                "id": "gyESiU8KR_HP"
            },
            "source": [
                "# Setup\n",
                "Let\u2019s start by making sure the right packages are installed and imported. We need to install the `mosaicml-streaming` package which installs the sufficient dependencies to run this tutorial."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "a3547314-ce67-4617-84b0-fafe366f82e4",
                    "showTitle": false,
                    "title": ""
                },
                "id": "QE2DHOK_R_HP",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "outputId": "271a7797-600b-4615-991d-0e76af052030"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (2023.6.0)\n",
                        "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
                        "Collecting transformers\n",
                        "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
                        "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
                        "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
                        "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
                        "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
                        "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
                        "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
                        "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
                        "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
                        "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
                        "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
                        "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
                        "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
                        "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
                        "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
                        "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
                        "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
                        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
                        "Installing collected packages: tokenizers, safetensors, transformers\n",
                        "Successfully installed safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"
                    ]
                }
            ],
            "source": [
                "%pip install --upgrade fsspec  datasets transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "115c2bc7-94d6-4ada-926c-fc5bdfd6c29c",
                    "showTitle": false,
                    "title": ""
                },
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "id": "PeEMBLaSR_HP",
                "outputId": "68050138-676e-4a5d-cbe3-da9d41a1782d"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Collecting git+https://github.com/XiaohanZhangCMU/streamingX.git@hackathon\n",
                        "  Cloning https://github.com/XiaohanZhangCMU/streamingX.git (to revision hackathon) to /tmp/pip-req-build-5lek3i6u\n",
                        "  Running command git clone --filter=blob:none --quiet https://github.com/XiaohanZhangCMU/streamingX.git /tmp/pip-req-build-5lek3i6u\n",
                        "  Running command git checkout -b hackathon --track origin/hackathon\n",
                        "  Switched to a new branch 'hackathon'\n",
                        "  Branch 'hackathon' set up to track remote branch 'hackathon' from 'origin'.\n",
                        "  Resolved https://github.com/XiaohanZhangCMU/streamingX.git to commit 9eaa43d745484bfb5c4d790bc94bdfd816a71a62\n",
                        "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
                        "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
                        "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
                        "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
                        "Collecting boto3<2,>=1.21.45 (from mosaicml-streaming==0.5.2)\n",
                        "  Downloading boto3-1.28.38-py3-none-any.whl (135 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting Brotli>=1.0.9 (from mosaicml-streaming==0.5.2)\n",
                        "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting google-cloud-storage>=2.9.0 (from mosaicml-streaming==0.5.2)\n",
                        "  Downloading google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: matplotlib<4,>=3.5.2 in /usr/local/lib/python3.10/dist-packages (from mosaicml-streaming==0.5.2) (3.7.1)\n",
                        "Collecting paramiko<4,>=2.11.0 (from mosaicml-streaming==0.5.2)\n",
                        "  Downloading paramiko-3.3.1-py3-none-any.whl (224 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting python-snappy<1,>=0.6.1 (from mosaicml-streaming==0.5.2)\n",
                        "  Downloading python_snappy-0.6.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (55 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: torch<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from mosaicml-streaming==0.5.2) (2.0.1+cu118)\n",
                        "Requirement already satisfied: torchtext>=0.10 in /usr/local/lib/python3.10/dist-packages (from mosaicml-streaming==0.5.2) (0.15.2)\n",
                        "Requirement already satisfied: torchvision>=0.10 in /usr/local/lib/python3.10/dist-packages (from mosaicml-streaming==0.5.2) (0.15.2+cu118)\n",
                        "Requirement already satisfied: tqdm<5,>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from mosaicml-streaming==0.5.2) (4.66.1)\n",
                        "Requirement already satisfied: transformers<5,>=4.21.3 in /usr/local/lib/python3.10/dist-packages (from mosaicml-streaming==0.5.2) (4.32.1)\n",
                        "Requirement already satisfied: xxhash<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mosaicml-streaming==0.5.2) (3.3.0)\n",
                        "Collecting zstd<2,>=1.5.2.5 (from mosaicml-streaming==0.5.2)\n",
                        "  Downloading zstd-1.5.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting oci<3,>=2.88 (from mosaicml-streaming==0.5.2)\n",
                        "  Downloading oci-2.111.0-py3-none-any.whl (22.8 MB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22.8/22.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting azure-storage-blob<13,>=12.0.0 (from mosaicml-streaming==0.5.2)\n",
                        "  Downloading azure_storage_blob-12.17.0-py3-none-any.whl (388 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting azure-storage-file-datalake<13,>=12.11.0 (from mosaicml-streaming==0.5.2)\n",
                        "  Downloading azure_storage_file_datalake-12.12.0-py3-none-any.whl (247 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting azure-identity>=1.13.0 (from mosaicml-streaming==0.5.2)\n",
                        "  Downloading azure_identity-1.14.0-py3-none-any.whl (160 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m160.2/160.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting azure-core<2.0.0,>=1.11.0 (from azure-identity>=1.13.0->mosaicml-streaming==0.5.2)\n",
                        "  Downloading azure_core-1.29.3-py3-none-any.whl (191 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m191.4/191.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.10/dist-packages (from azure-identity>=1.13.0->mosaicml-streaming==0.5.2) (41.0.3)\n",
                        "Collecting msal<2.0.0,>=1.20.0 (from azure-identity>=1.13.0->mosaicml-streaming==0.5.2)\n",
                        "  Downloading msal-1.23.0-py2.py3-none-any.whl (90 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting msal-extensions<2.0.0,>=0.3.0 (from azure-identity>=1.13.0->mosaicml-streaming==0.5.2)\n",
                        "  Downloading msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\n",
                        "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob<13,>=12.0.0->mosaicml-streaming==0.5.2) (4.7.1)\n",
                        "Collecting isodate>=0.6.1 (from azure-storage-blob<13,>=12.0.0->mosaicml-streaming==0.5.2)\n",
                        "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting botocore<1.32.0,>=1.31.38 (from boto3<2,>=1.21.45->mosaicml-streaming==0.5.2)\n",
                        "  Downloading botocore-1.31.38-py3-none-any.whl (11.2 MB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.21.45->mosaicml-streaming==0.5.2)\n",
                        "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
                        "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3<2,>=1.21.45->mosaicml-streaming==0.5.2)\n",
                        "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (2.17.3)\n",
                        "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (2.11.1)\n",
                        "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (2.3.3)\n",
                        "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (2.5.0)\n",
                        "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (2.31.0)\n",
                        "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.5.2->mosaicml-streaming==0.5.2) (1.1.0)\n",
                        "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.5.2->mosaicml-streaming==0.5.2) (0.11.0)\n",
                        "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.5.2->mosaicml-streaming==0.5.2) (4.42.1)\n",
                        "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.5.2->mosaicml-streaming==0.5.2) (1.4.4)\n",
                        "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.5.2->mosaicml-streaming==0.5.2) (1.23.5)\n",
                        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.5.2->mosaicml-streaming==0.5.2) (23.1)\n",
                        "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.5.2->mosaicml-streaming==0.5.2) (9.4.0)\n",
                        "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.5.2->mosaicml-streaming==0.5.2) (3.1.1)\n",
                        "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.5.2->mosaicml-streaming==0.5.2) (2.8.2)\n",
                        "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from oci<3,>=2.88->mosaicml-streaming==0.5.2) (2023.7.22)\n",
                        "Requirement already satisfied: pyOpenSSL<24.0.0,>=17.5.0 in /usr/local/lib/python3.10/dist-packages (from oci<3,>=2.88->mosaicml-streaming==0.5.2) (23.2.0)\n",
                        "Requirement already satisfied: pytz>=2016.10 in /usr/local/lib/python3.10/dist-packages (from oci<3,>=2.88->mosaicml-streaming==0.5.2) (2023.3)\n",
                        "Collecting circuitbreaker<2.0.0,>=1.3.1 (from oci<3,>=2.88->mosaicml-streaming==0.5.2)\n",
                        "  Downloading circuitbreaker-1.4.0.tar.gz (9.7 kB)\n",
                        "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "Collecting bcrypt>=3.2 (from paramiko<4,>=2.11.0->mosaicml-streaming==0.5.2)\n",
                        "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting pynacl>=1.5 (from paramiko<4,>=2.11.0->mosaicml-streaming==0.5.2)\n",
                        "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.10->mosaicml-streaming==0.5.2) (3.12.2)\n",
                        "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.10->mosaicml-streaming==0.5.2) (1.12)\n",
                        "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.10->mosaicml-streaming==0.5.2) (3.1)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.10->mosaicml-streaming==0.5.2) (3.1.2)\n",
                        "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.10->mosaicml-streaming==0.5.2) (2.0.0)\n",
                        "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1.10->mosaicml-streaming==0.5.2) (3.27.2)\n",
                        "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1.10->mosaicml-streaming==0.5.2) (16.0.6)\n",
                        "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext>=0.10->mosaicml-streaming==0.5.2) (0.6.1)\n",
                        "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext>=0.10->mosaicml-streaming==0.5.2) (2.0.4)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.21.3->mosaicml-streaming==0.5.2) (0.16.4)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.21.3->mosaicml-streaming==0.5.2) (6.0.1)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.21.3->mosaicml-streaming==0.5.2) (2023.6.3)\n",
                        "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.21.3->mosaicml-streaming==0.5.2) (0.13.3)\n",
                        "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.21.3->mosaicml-streaming==0.5.2) (0.3.3)\n",
                        "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.11.0->azure-identity>=1.13.0->mosaicml-streaming==0.5.2) (1.16.0)\n",
                        "Collecting urllib3>=1.25 (from torchdata==0.6.1->torchtext>=0.10->mosaicml-streaming==0.5.2)\n",
                        "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.5->azure-identity>=1.13.0->mosaicml-streaming==0.5.2) (1.15.1)\n",
                        "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (1.60.0)\n",
                        "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (3.20.3)\n",
                        "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (5.3.1)\n",
                        "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (0.3.0)\n",
                        "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (4.9)\n",
                        "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (1.5.0)\n",
                        "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers<5,>=4.21.3->mosaicml-streaming==0.5.2) (2023.6.0)\n",
                        "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /usr/lib/python3/dist-packages (from msal<2.0.0,>=1.20.0->azure-identity>=1.13.0->mosaicml-streaming==0.5.2) (2.3.0)\n",
                        "Collecting portalocker<3,>=1.0 (from msal-extensions<2.0.0,>=0.3.0->azure-identity>=1.13.0->mosaicml-streaming==0.5.2)\n",
                        "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (3.2.0)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (3.4)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.10->mosaicml-streaming==0.5.2) (2.1.3)\n",
                        "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.10->mosaicml-streaming==0.5.2) (1.3.0)\n",
                        "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity>=1.13.0->mosaicml-streaming==0.5.2) (2.21)\n",
                        "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage>=2.9.0->mosaicml-streaming==0.5.2) (0.5.0)\n",
                        "Building wheels for collected packages: mosaicml-streaming, circuitbreaker\n",
                        "  Building wheel for mosaicml-streaming (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
                        "  Created wheel for mosaicml-streaming: filename=mosaicml_streaming-0.5.2-py3-none-any.whl size=183248 sha256=af04d15c64aeed0be9fc96260bbd7c8a0abfa6eb3c98910f2adeedc85faa0489\n",
                        "  Stored in directory: /tmp/pip-ephem-wheel-cache-l_6ng3ze/wheels/27/49/b1/e65ea3d772b70835f637baf9d1dd61734d0f692ddabeb7481e\n",
                        "  Building wheel for circuitbreaker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "  Created wheel for circuitbreaker: filename=circuitbreaker-1.4.0-py3-none-any.whl size=7518 sha256=5a2c4a92a10919c3b1a922ffd8955ffac5c674f08242f705db7beca29d69f59f\n",
                        "  Stored in directory: /root/.cache/pip/wheels/3c/be/64/266b6ce2ef1130de5e419f04805acbb2df5a4ab1b91348f25b\n",
                        "Successfully built mosaicml-streaming circuitbreaker\n",
                        "Installing collected packages: zstd, python-snappy, circuitbreaker, Brotli, urllib3, portalocker, jmespath, isodate, bcrypt, pynacl, botocore, s3transfer, paramiko, azure-core, oci, msal, boto3, azure-storage-blob, msal-extensions, google-cloud-storage, azure-storage-file-datalake, azure-identity, mosaicml-streaming\n",
                        "  Attempting uninstall: urllib3\n",
                        "    Found existing installation: urllib3 2.0.4\n",
                        "    Uninstalling urllib3-2.0.4:\n",
                        "      Successfully uninstalled urllib3-2.0.4\n",
                        "  Attempting uninstall: google-cloud-storage\n",
                        "    Found existing installation: google-cloud-storage 2.8.0\n",
                        "    Uninstalling google-cloud-storage-2.8.0:\n",
                        "      Successfully uninstalled google-cloud-storage-2.8.0\n",
                        "Successfully installed Brotli-1.0.9 azure-core-1.29.3 azure-identity-1.14.0 azure-storage-blob-12.17.0 azure-storage-file-datalake-12.12.0 bcrypt-4.0.1 boto3-1.28.38 botocore-1.31.38 circuitbreaker-1.4.0 google-cloud-storage-2.10.0 isodate-0.6.1 jmespath-1.0.1 mosaicml-streaming-0.5.2 msal-1.23.0 msal-extensions-1.0.0 oci-2.111.0 paramiko-3.3.1 portalocker-2.7.0 pynacl-1.5.0 python-snappy-0.6.1 s3transfer-0.6.2 urllib3-1.26.16 zstd-1.5.5.1\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.colab-display-data+json": {
                            "pip_warning": {
                                "packages": [
                                    "google",
                                    "urllib3"
                                ]
                            }
                        }
                    },
                    "metadata": {}
                }
            ],
            "source": [
                "!pip uninstall -y mosaicml-streaming"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install pyspark==3.4.1"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "zETa4qH0TnPE",
                "outputId": "4fc61a92-57f7-4404-d441-ab2776d46d15"
            },
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Collecting pyspark==3.4.1\n",
                        "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
                        "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.1) (0.10.9.7)\n",
                        "Building wheels for collected packages: pyspark\n",
                        "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=6f20b50f9281ea800ca83dcfaf915a5201986ce6577ce306f62de5ca90bcb0f9\n",
                        "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
                        "Successfully built pyspark\n",
                        "Installing collected packages: pyspark\n",
                        "Successfully installed pyspark-3.4.1\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import shutil\n",
                "from typing import Any, Sequence, Dict, Iterable, Optional\n",
                "from pyspark.sql import SparkSession\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from tempfile import mkdtemp\n",
                "import datasets as hf_datasets\n",
                "from transformers import AutoTokenizer, PreTrainedTokenizerBase"
            ],
            "metadata": {
                "id": "zjlPAIBfhSbg"
            },
            "execution_count": 16,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "We\u2019ll be using Streaming\u2019s `dataframeToMDS()` method which converts the dataframe into the streaming MDS format."
            ],
            "metadata": {
                "id": "G1I2CtGpRk40"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from streaming.base.converters import dataframeToMDS"
            ],
            "metadata": {
                "id": "uzYHe6yYRzyV"
            },
            "execution_count": 3,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "42e9ffbc-52a7-479d-b3a5-608d044d1f6a",
                    "showTitle": false,
                    "title": ""
                },
                "id": "Un4G3VdgR_HQ"
            },
            "source": [
                "## [Basic use-case] Convert spark dataframe to MDS format\n",
                "**Steps:**\n",
                "1. Create a Synthetic NLP dataset.\n",
                "2. Store the above dataset as a parquet file.\n",
                "3. Load the parquet file as spark dataframe.\n",
                "4. Convert the spark dataframe to MDS format.\n",
                "5. Load the MDS dataset and look at the output."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "90184dc8-cb85-4921-bc8e-e1525c6ee212",
                    "showTitle": false,
                    "title": ""
                },
                "id": "oKA2TTJ-R_HQ"
            },
            "source": [
                "### Create a Synthetic NLP dataset\n",
                "\n",
                "In this tutorial, we will be creating a synthetic number-saying dataset, i.e. converting a numbers from digits to words, for example, number `123` would spell as `one hundred twenty three`. The numbers are generated sequentially with a random positive/negative prefix sign.\n",
                "\n",
                "Let\u2019s import a utility functions to generate those synthetic number-saying dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "2e58eb05-4271-4ea4-b7ce-2e5466a1405b",
                    "showTitle": false,
                    "title": ""
                },
                "id": "SMirmZEqR_HQ"
            },
            "outputs": [],
            "source": [
                "class NumberAndSayDataset:\n",
                "    \"\"\"Generate a synthetic number-saying dataset.\n",
                "\n",
                "    Converting a numbers from digits to words, for example, number 123 would spell as\n",
                "    `one hundred twenty three`. The numbers are generated randomly and it supports a number\n",
                "    up-to positive/negative approximately 99 Millions.\n",
                "\n",
                "    Args:\n",
                "        num_samples (int): number of samples. Defaults to 100.\n",
                "        column_names list[str]: A list of features' and target name. Defaults to ['number',\n",
                "            'words'].\n",
                "        seed (int): seed value for deterministic randomness.\n",
                "    \"\"\"\n",
                "\n",
                "    ones = (\n",
                "        'zero one two three four five six seven eight nine ten eleven twelve thirteen fourteen ' +\n",
                "        'fifteen sixteen seventeen eighteen nineteen').split()\n",
                "\n",
                "    tens = 'twenty thirty forty fifty sixty seventy eighty ninety'.split()\n",
                "\n",
                "    def __init__(self,\n",
                "                 num_samples: int = 100,\n",
                "                 column_names: list[str] = ['number', 'words'],\n",
                "                 seed: int = 987) -> None:\n",
                "        self.num_samples = num_samples\n",
                "        self.column_encodings = ['int', 'str']\n",
                "        self.column_sizes = [8, None]\n",
                "        self.column_names = column_names\n",
                "        self._index = 0\n",
                "        self.seed = seed\n",
                "\n",
                "    def __len__(self) -> int:\n",
                "        return self.num_samples\n",
                "\n",
                "    def _say(self, i: int) -> list[str]:\n",
                "        if i < 0:\n",
                "            return ['negative'] + self._say(-i)\n",
                "        elif i <= 19:\n",
                "            return [self.ones[i]]\n",
                "        elif i < 100:\n",
                "            return [self.tens[i // 10 - 2]] + ([self.ones[i % 10]] if i % 10 else [])\n",
                "        elif i < 1_000:\n",
                "            return [self.ones[i // 100], 'hundred'] + (self._say(i % 100) if i % 100 else [])\n",
                "        elif i < 1_000_000:\n",
                "            return self._say(i // 1_000) + ['thousand'\n",
                "                                           ] + (self._say(i % 1_000) if i % 1_000 else [])\n",
                "        elif i < 1_000_000_000:\n",
                "            return self._say(\n",
                "                i // 1_000_000) + ['million'] + (self._say(i % 1_000_000) if i % 1_000_000 else [])\n",
                "        else:\n",
                "            assert False\n",
                "\n",
                "    def _get_number(self) -> int:\n",
                "        sign = (np.random.random() < 0.8) * 2 - 1\n",
                "        mag = 10**np.random.uniform(1, 4) - 10\n",
                "        return sign * int(mag**2)\n",
                "\n",
                "    def __iter__(self):\n",
                "        return self\n",
                "\n",
                "    def __next__(self) -> dict[str, Any]:\n",
                "        if self._index >= self.num_samples:\n",
                "            raise StopIteration\n",
                "        number = self._get_number()\n",
                "        words = ' '.join(self._say(number))\n",
                "        self._index += 1\n",
                "        return {\n",
                "            self.column_names[0]: number,\n",
                "            self.column_names[1]: words,\n",
                "        }\n",
                "\n",
                "    @property\n",
                "    def seed(self) -> int:\n",
                "        return self._seed\n",
                "\n",
                "    @seed.setter\n",
                "    def seed(self, value: int) -> None:\n",
                "        self._seed = value  # pyright: ignore\n",
                "        np.random.seed(self._seed)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Store the dataset as a parquet file"
            ],
            "metadata": {
                "id": "c4tvDe8hTeMS"
            }
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "89707251-35c0-4c17-841d-2cc026e6f96f",
                    "showTitle": false,
                    "title": ""
                },
                "id": "ysofCNb3R_HR"
            },
            "outputs": [],
            "source": [
                "# Create a temporary directory\n",
                "local_dir = mkdtemp()\n",
                "\n",
                "syn_dataset = NumberAndSayDataset()\n",
                "df = pd.DataFrame.from_dict([record for record in syn_dataset])\n",
                "df.to_parquet(os.path.join(local_dir, 'synthetic_dataset.parquet'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "e9e12fcc-fa78-4635-9e62-fb67ca5f520f",
                    "showTitle": false,
                    "title": ""
                },
                "id": "4hvJgJGxR_HR"
            },
            "source": [
                "### Load the parquet file as spark dataframe"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "spark = SparkSession.builder.getOrCreate()\n",
                "pdf = spark.read.parquet(os.path.join(local_dir, 'synthetic_dataset.parquet'))"
            ],
            "metadata": {
                "id": "10e4ZrH3he0q"
            },
            "execution_count": 6,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "9a43d639-c0a8-438a-9c56-f748b6ca79ad",
                    "showTitle": false,
                    "title": ""
                },
                "id": "8JfeOLqWR_HS"
            },
            "source": [
                "Take a peek at the spark dataframe"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "pdf.show(5, truncate=False)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "iGkjgXtDhk6g",
                "outputId": "7b217db9-09e3-4ef8-f171-245d09cc0119"
            },
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+---------+-----------------------------------------------------------------------------------+\n",
                        "|number   |words                                                                              |\n",
                        "+---------+-----------------------------------------------------------------------------------+\n",
                        "|19660    |nineteen thousand six hundred sixty                                                |\n",
                        "|-52186   |negative fifty two thousand one hundred eighty six                                 |\n",
                        "|-32652133|negative thirty two million six hundred fifty two thousand one hundred thirty three|\n",
                        "|-59717966|negative fifty nine million seven hundred seventeen thousand nine hundred sixty six|\n",
                        "|74       |seventy four                                                                       |\n",
                        "+---------+-----------------------------------------------------------------------------------+\n",
                        "only showing top 5 rows\n",
                        "\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "2a2468b8-9538-42e6-b0c6-937a3de92210",
                    "showTitle": false,
                    "title": ""
                },
                "id": "pt5MYIrmR_HS"
            },
            "source": [
                "### Convert the spark dataframe to MDS format"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "aad931d4-8b97-4d5b-9c40-0f35e715c7b4",
                    "showTitle": false,
                    "title": ""
                },
                "id": "zRghnT00R_HS",
                "outputId": "afd3b4f2-0c2c-4296-8def-e7ebb1f2f9d0",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                }
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "('/tmp/tmpe_t_zz_f/mds', 0)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "source": [
                "# Empty the MDS output directory\n",
                "out_path = os.path.join(local_dir, 'mds')\n",
                "shutil.rmtree(out_path, ignore_errors=True)\n",
                "\n",
                "# To Xiaohan: Can we skip passing mds_kwargs for basic use-case and let it infer by dataframeToMDS method ?\n",
                "mds_kwargs = {'out': out_path, 'columns': {'number': 'int64', 'words':'str'}}\n",
                "\n",
                "# Convert the dataset to an MDS format. It divides the dataframe into 4 parts, one parts per worker and merge the `index.json` from 4 sub-parts into one in a parent directory.\n",
                "dataframeToMDS(pdf.repartition(4), merge_index=True, mds_kwargs=mds_kwargs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "327543b6-b2c8-4c60-bff3-bb4e5a980a28",
                    "showTitle": false,
                    "title": ""
                },
                "id": "MdKpnmkhR_HS"
            },
            "source": [
                "Let's check file structures in the output MDS dataset. One can see four directories and one `index.json` file. The `index.json` file contains the meta-data information about all four sub-directories."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "268c137e-6e38-4f31-8e51-0adfe3c82c44",
                    "showTitle": false,
                    "title": ""
                },
                "id": "CDbPsmUeR_HS",
                "outputId": "669453bb-634a-4ab5-ef7d-d9b95cb90846",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                }
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "10  11\t12  9  index.json\n"
                    ]
                }
            ],
            "source": [
                "!ls {out_path}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "6d6aa896-9d5a-4763-aaf4-197f9170ec27",
                    "showTitle": false,
                    "title": ""
                },
                "id": "S6j3pr_IR_HS"
            },
            "source": [
                "### Load the MDS dataset using StreamingDataset and look at the output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "ad4359ac-1652-48bd-b4db-ea034e180e01",
                    "showTitle": false,
                    "title": ""
                },
                "id": "qfpr2Df3R_HS",
                "outputId": "503ac3b0-46ed-4b96-e171-8929f9662d40",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                }
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'number': tensor([    6678, 95991561]), 'words': ['six thousand six hundred seventy eight', 'ninety five million nine hundred ninety one thousand five hundred sixty one']}\n",
                        "{'number': tensor([746126,    151]), 'words': ['seven hundred forty six thousand one hundred twenty six', 'one hundred fifty one']}\n",
                        "{'number': tensor([22867,  2184]), 'words': ['twenty two thousand eight hundred sixty seven', 'two thousand one hundred eighty four']}\n",
                        "{'number': tensor([   0, 7555]), 'words': ['zero', 'seven thousand five hundred fifty five']}\n",
                        "{'number': tensor([    4805, 13256098]), 'words': ['four thousand eight hundred five', 'thirteen million two hundred fifty six thousand ninety eight']}\n",
                        "{'number': tensor([  2741583, -12578223]), 'words': ['two million seven hundred forty one thousand five hundred eighty three', 'negative twelve million five hundred seventy eight thousand two hundred twenty three']}\n",
                        "{'number': tensor([197716,    -34]), 'words': ['one hundred ninety seven thousand seven hundred sixteen', 'negative thirty four']}\n",
                        "{'number': tensor([25617032,  3697464]), 'words': ['twenty five million six hundred seventeen thousand thirty two', 'three million six hundred ninety seven thousand four hundred sixty four']}\n",
                        "{'number': tensor([37870542,    13005]), 'words': ['thirty seven million eight hundred seventy thousand five hundred forty two', 'thirteen thousand five']}\n",
                        "{'number': tensor([  163259, 33049178]), 'words': ['one hundred sixty three thousand two hundred fifty nine', 'thirty three million forty nine thousand one hundred seventy eight']}\n",
                        "{'number': tensor([1715560, 1065554]), 'words': ['one million seven hundred fifteen thousand five hundred sixty', 'one million sixty five thousand five hundred fifty four']}\n"
                    ]
                }
            ],
            "source": [
                "from torch.utils.data import DataLoader\n",
                "import streaming\n",
                "from streaming import StreamingDataset\n",
                "\n",
                "# clean stale shared memory if any\n",
                "streaming.base.util.clean_stale_shared_memory()\n",
                "\n",
                "dataset = StreamingDataset(local=out_path, remote=None, batch_size=2, predownload=4)\n",
                "\n",
                "dataloader = DataLoader(dataset, batch_size=2, num_workers=1)\n",
                "\n",
                "for i, data in enumerate(dataloader):\n",
                "    print(data)\n",
                "    # Display only first 10 batches\n",
                "    if i == 10:\n",
                "        break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "84e7df6b-b890-45ad-93de-00903a59c058",
                    "showTitle": false,
                    "title": ""
                },
                "id": "Uo1jLj1dR_HS"
            },
            "source": [
                "## [Advanced use-case] convert spark dataframe into tokenized format and convert to MDS format\n",
                "**Steps:**\n",
                "1. [Same as above] Create a Synthetic NLP dataset.\n",
                "2. [Same as above] Store the above dataset as a parquet file.\n",
                "3. [Same as above] Load the parquet file as spark dataframe.\n",
                "4. Create a user defined function which modifies the dataframe\n",
                "4. Convert the modified data into MDS format.\n",
                "5. Load the MDS dataset and look at the output."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "57ffd58d-118b-485b-9f5c-d42ecc6f81ad",
                    "showTitle": false,
                    "title": ""
                },
                "id": "geRPrzMhR_HS"
            },
            "source": [
                "### Create a user defined function which modifies the dataframe\n",
                "\n",
                "The user defined function should be an iterable function and it must yield an output as a dictionary with `key` as the column name and `value` as the output of that column. For example, in this tutorial, the `key` is `tokens` and `value` is the tokenized output in bytes. If an iterable function is defined, the user takes the full responsibility of providing the correct `columns` argument, in the case below, it should be\n",
                "\n",
                "columns={'tokens': 'bytes'}\n",
                "\n",
                "where `tokens` is the key created by the udf_iterator, and `bytes` represents the format of the field so that MDS chooses the proper encoding method."
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "Take a peek at the spark dataframe"
            ],
            "metadata": {
                "id": "P1P7BsXsaDCy"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "pdf.show(5, truncate=False)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "mU5qfafGzKsQ",
                "outputId": "1c8ec69d-7a1f-40fc-ac96-7e219c0136b8"
            },
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+---------+-----------------------------------------------------------------------------------+\n",
                        "|number   |words                                                                              |\n",
                        "+---------+-----------------------------------------------------------------------------------+\n",
                        "|19660    |nineteen thousand six hundred sixty                                                |\n",
                        "|-52186   |negative fifty two thousand one hundred eighty six                                 |\n",
                        "|-32652133|negative thirty two million six hundred fifty two thousand one hundred thirty three|\n",
                        "|-59717966|negative fifty nine million seven hundred seventeen thousand nine hundred sixty six|\n",
                        "|74       |seventy four                                                                       |\n",
                        "+---------+-----------------------------------------------------------------------------------+\n",
                        "only showing top 5 rows\n",
                        "\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Convert the spark dataframe to MDS format\n",
                "This time we supply the user defined iterable function and the associated function arguments. For the purpose of demonstration, the user defined tokenization function `pandas_processing_fn` is largely simplified. For practical applications, the users may want to have more involved preprocessing steps. For concatenation dataset and more process examples, users are referred to [Mosaic's LLM Foundry](https://github.com/mosaicml/llm-foundry/blob/main/llmfoundry/data/data.py)."
            ],
            "metadata": {
                "id": "JMeyEPoSaGgH"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import warnings\n",
                "from typing import Dict, Iterable, Union\n",
                "import datasets as hf_datasets\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from torch.utils.data import IterableDataset\n",
                "from transformers import PreTrainedTokenizerBase\n",
                "\n",
                "\n",
                "def pandas_processing_fn(df: pd.DataFrame, **args) -> Iterable[Dict[str, bytes]]:\n",
                "    \"\"\"\n",
                "    Parameters:\n",
                "    -----------\n",
                "    df : pandas.DataFrame\n",
                "        The input pandas DataFrame that needs to be processed.\n",
                "\n",
                "    **args : keyword arguments\n",
                "        Additional arguments to be passed to the 'process_some_data' function during processing.\n",
                "\n",
                "    Returns:\n",
                "    --------\n",
                "    iterable obj\n",
                "    \"\"\"\n",
                "    hf_dataset = hf_datasets.Dataset.from_pandas(df=df, split=args['split'])\n",
                "    tokenizer = AutoTokenizer.from_pretrained(args['tokenizer'])\n",
                "    # we will enforce length, so suppress warnings about sequences too long for the model\n",
                "    tokenizer.model_max_length = int(1e30)\n",
                "    max_length = args['concat_tokens']\n",
                "\n",
                "    for sample in hf_dataset:\n",
                "\n",
                "        buffer = []\n",
                "        for sample in hf_dataset:\n",
                "            encoded = tokenizer(sample['words'],\n",
                "                                truncation=False,\n",
                "                                padding=False)\n",
                "            iids = encoded['input_ids']\n",
                "            buffer = buffer + iids\n",
                "            while len(buffer) >= max_length:\n",
                "                concat_sample = buffer[:max_length]\n",
                "                buffer = []\n",
                "                yield {\n",
                "                    # convert to bytes to store in MDS binary format\n",
                "                    'tokens': np.asarray(concat_sample).tobytes()\n",
                "                }"
            ],
            "metadata": {
                "id": "H7Z5kjNie4ZR"
            },
            "execution_count": 17,
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "b0b3cfd1-9753-44ca-9b03-55f29c60fb56",
                    "showTitle": false,
                    "title": ""
                },
                "id": "R9Xxr0CDR_HT",
                "outputId": "4d5f4b24-f963-4b4e-d532-f8c3a78baeee",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                }
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "WARNING:streaming.base.converters.dataframe_to_mds:With udf_iterable defined, it's up to the user's descretion to provide mds_kwargs[columns'\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "('/tmp/tmpe_t_zz_f/mds', 0)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 18
                }
            ],
            "source": [
                "# Empty the MDS output directory\n",
                "out_path = os.path.join(local_dir, 'mds')\n",
                "shutil.rmtree(out_path, ignore_errors=True)\n",
                "\n",
                "# Provide a MDS keyword args. Ensure `columns` field maps the output from iterable function (Tokenizer in this example)\n",
                "mds_kwargs = {'out': out_path, 'columns': {'tokens': 'bytes'}}\n",
                "\n",
                "# Tokenizer arguments\n",
                "udf_kwargs = {\n",
                "    'concat_tokens': 4,\n",
                "    'tokenizer': 'EleutherAI/gpt-neox-20b',\n",
                "    'eos_text': '<|endoftext|>',\n",
                "    'compression': 'zstd',\n",
                "    'split': 'train',\n",
                "    'no_wrap': False,\n",
                "    'bos_text': '',\n",
                "}\n",
                "\n",
                "# Convert the dataset to an MDS format. It fetches sample from dataframe, tokenize it, and then convert to MDS format.\n",
                "# It divides the dataframe into 4 parts, one parts per worker and merge the `index.json` from 4 sub-parts into one in a parent directory.\n",
                "dataframeToMDS(pdf.repartition(4), merge_index=True, mds_kwargs=mds_kwargs, udf_iterable=pandas_processing_fn, udf_kwargs=udf_kwargs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "8c3ea1b0-cab4-4d0b-b4a2-ed30dae39959",
                    "showTitle": false,
                    "title": ""
                },
                "id": "Vady_sp9R_HT"
            },
            "source": [
                "Let's check file structures in the output MDS dataset. One can see four directories and one `index.json` file. The `index.json` file contains the meta-data information about all four sub-directories."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "a071fe82-8d1e-4dd4-9e34-8a0dcbee32e0",
                    "showTitle": false,
                    "title": ""
                },
                "id": "VUP5VsILR_HT",
                "outputId": "8efae289-7401-46a9-c476-fc9392cec00c",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                }
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "41  42\t43  44\tindex.json\n"
                    ]
                }
            ],
            "source": [
                "!ls {out_path}"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "!cat {out_path +'/index.json'}"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "WVR9ld1vl5VX",
                "outputId": "a2234426-0563-43de-bee4-8d56e3c3a895"
            },
            "execution_count": 20,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{\"version\": 2, \"shards\": [{\"column_encodings\": [\"bytes\"], \"column_names\": [\"tokens\"], \"column_sizes\": [null], \"compression\": null, \"format\": \"mds\", \"hashes\": [], \"raw_data\": {\"basename\": \"41/shard.00000.mds\", \"bytes\": 23181, \"hashes\": {}}, \"samples\": 575, \"size_limit\": 67108864, \"version\": 2, \"zip_data\": null}, {\"column_encodings\": [\"bytes\"], \"column_names\": [\"tokens\"], \"column_sizes\": [null], \"compression\": null, \"format\": \"mds\", \"hashes\": [], \"raw_data\": {\"basename\": \"42/shard.00000.mds\", \"bytes\": 24181, \"hashes\": {}}, \"samples\": 600, \"size_limit\": 67108864, \"version\": 2, \"zip_data\": null}, {\"column_encodings\": [\"bytes\"], \"column_names\": [\"tokens\"], \"column_sizes\": [null], \"compression\": null, \"format\": \"mds\", \"hashes\": [], \"raw_data\": {\"basename\": \"43/shard.00000.mds\", \"bytes\": 22181, \"hashes\": {}}, \"samples\": 550, \"size_limit\": 67108864, \"version\": 2, \"zip_data\": null}, {\"column_encodings\": [\"bytes\"], \"column_names\": [\"tokens\"], \"column_sizes\": [null], \"compression\": null, \"format\": \"mds\", \"hashes\": [], \"raw_data\": {\"basename\": \"44/shard.00000.mds\", \"bytes\": 23181, \"hashes\": {}}, \"samples\": 575, \"size_limit\": 67108864, \"version\": 2, \"zip_data\": null}]}"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "f84d61f1-9f09-484c-97cf-1f5ee1e8a214",
                    "showTitle": false,
                    "title": ""
                },
                "id": "q7r3o-mKR_HT"
            },
            "source": [
                "### Load the MDS dataset using StreamingDataset and look at the output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "ee73abb8-2261-484d-93d5-722b67090ed6",
                    "showTitle": false,
                    "title": ""
                },
                "id": "gQSmFIq6R_HT",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "outputId": "75655637-89a0-4e3d-f4a7-2e0ab5bb3ded"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'tokens': [b'\\x1eC\\x00\\x00\\x00\\x00\\x00\\x00N\\x1f\\x00\\x00\\x00\\x00\\x00\\x00\\xf0\\n\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00', b'U0\\x00\\x00\\x00\\x00\\x00\\x00\\xcc\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\xb9m\\x00\\x00\\x00\\x00\\x00\\x00']}\n",
                        "{'tokens': [b'U0\\x00\\x00\\x00\\x00\\x00\\x00EL\\x00\\x00\\x00\\x00\\x00\\x00N\\x1f\\x00\\x00\\x00\\x00\\x00\\x00\\xf77\\x00\\x00\\x00\\x00\\x00\\x00', b'O\\x00\\x00\\x00\\x00\\x00\\x00\\x00s\\x1e\\x00\\x00\\x00\\x00\\x00\\x00\\xc9%\\x00\\x00\\x00\\x00\\x00\\x00N\\x1f\\x00\\x00\\x00\\x00\\x00\\x00']}\n",
                        "{'tokens': [b\"'/\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x975\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\x02\\x00\\x00\\x00\\x00\\x00\\x00\", b'U0\\x00\\x00\\x00\\x00\\x00\\x00\\xf8(\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xe1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00']}\n",
                        "{'tokens': [b'\\x8a\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xdf\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\xcc\\x06\\x00\\x00\\x00\\x00\\x00\\x00N\\x1f\\x00\\x00\\x00\\x00\\x00\\x00', b'\\xd44\\x00\\x00\\x00\\x00\\x00\\x00\\xe1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xf0\\n\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00']}\n",
                        "{'tokens': [b'\\xd00\\x00\\x00\\x00\\x00\\x00\\x00N\\x1f\\x00\\x00\\x00\\x00\\x00\\x00\\xda\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00', b'O\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8ac\\x00\\x00\\x00\\x00\\x00\\x00<\\n\\x00\\x00\\x00\\x00\\x00\\x00\\xe1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00']}\n",
                        "{'tokens': [b'\\xad$\\x00\\x00\\x00\\x00\\x00\\x00\\xe1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xe5\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00', b'\\x13\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x975\\x00\\x00\\x00\\x00\\x00\\x00E\\x02\\x00\\x00\\x00\\x00\\x00\\x00']}\n",
                        "{'tokens': [b'\\x13\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\xbc\\x7f\\x00\\x00\\x00\\x00\\x00\\x00\\xe5\\x13\\x00\\x00\\x00\\x00\\x00\\x00', b'\\xad$\\x00\\x00\\x00\\x00\\x00\\x00N\\x1f\\x00\\x00\\x00\\x00\\x00\\x00E\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00']}\n",
                        "{'tokens': [b'\\x13\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xe1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00<\\n\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00', b'\\xad$\\x00\\x00\\x00\\x00\\x00\\x00N\\x1f\\x00\\x00\\x00\\x00\\x00\\x00\\xf0\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00']}\n",
                        "{'tokens': [b'\\x8a\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xdf\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\xe5\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\xe1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00', b'U0\\x00\\x00\\x00\\x00\\x00\\x00\\xf0\\n\\x00\\x00\\x00\\x00\\x00\\x00\\xe1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00!\\x1d\\x00\\x00\\x00\\x00\\x00\\x00']}\n",
                        "{'tokens': [b'\\x13\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\xbc\\x7f\\x00\\x00\\x00\\x00\\x00\\x00E\\x02\\x00\\x00\\x00\\x00\\x00\\x00', b'U0\\x00\\x00\\x00\\x00\\x00\\x00\\xf0\\n\\x00\\x00\\x00\\x00\\x00\\x00\\xe1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00!\\x1d\\x00\\x00\\x00\\x00\\x00\\x00']}\n",
                        "{'tokens': [b'\\x13\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\xbc\\x7f\\x00\\x00\\x00\\x00\\x00\\x00E\\x02\\x00\\x00\\x00\\x00\\x00\\x00', b'U0\\x00\\x00\\x00\\x00\\x00\\x00\\xf0\\n\\x00\\x00\\x00\\x00\\x00\\x00\\xe1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00!\\x1d\\x00\\x00\\x00\\x00\\x00\\x00']}\n"
                    ]
                }
            ],
            "source": [
                "from torch.utils.data import DataLoader\n",
                "import streaming\n",
                "from streaming import StreamingDataset\n",
                "\n",
                "# clean stale shared memory if any\n",
                "streaming.base.util.clean_stale_shared_memory()\n",
                "\n",
                "dataset = StreamingDataset(local=out_path, remote=None, batch_size=2, predownload=4)\n",
                "\n",
                "dataloader = DataLoader(dataset, batch_size=2, num_workers=1)\n",
                "\n",
                "for i, data in enumerate(dataloader):\n",
                "    print(data)\n",
                "    # Display only first 10 batches\n",
                "    if i == 10:\n",
                "        break"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Cleanup\n",
                "\n",
                "That's it. No need to hang on to the files created by the tutorial..."
            ],
            "metadata": {
                "id": "xcVwdCL_bcg8"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "shutil.rmtree(out_path, ignore_errors=True)\n",
                "shutil.rmtree(local_dir, ignore_errors=True)"
            ],
            "metadata": {
                "id": "Wa_ZGkBckq-b"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "## What next?\n",
                "\n",
                "You've now seen an in-depth look at how to convert spark dataframe to MDS format and load the same MDS dataset for model training or for your personalized use-case.\n",
                "\n",
                "To continue learning about Streaming, please continue to explore our examples!"
            ],
            "metadata": {
                "id": "uia53VVabf5P"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Come get involved with MosaicML!\n",
                "\n",
                "We'd love for you to get involved with the MosaicML community in any of these ways:\n",
                "\n",
                "### [Star Streaming on GitHub](https://github.com/mosaicml/streaming)\n",
                "\n",
                "Help make others aware of our work by [starring Streaming on GitHub](https://github.com/mosaicml/streaming).\n",
                "\n",
                "### [Join the MosaicML Slack](https://mosaicml.me/slack)\n",
                "\n",
                "Head on over to the [MosaicML slack](https://mosaicml.me/slack) to join other ML efficiency enthusiasts. Come for the paper discussions, stay for the memes!\n",
                "\n",
                "### Contribute to Streaming\n",
                "\n",
                "Is there a bug you noticed or a feature you'd like? File an [issue](https://github.com/mosaicml/streaming/issues) or make a [pull request](https://github.com/mosaicml/streaming/pulls)!"
            ],
            "metadata": {
                "id": "N0LodDjmbimn"
            }
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {
                "id": "FkDyaa4VbkdP"
            },
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "application/vnd.databricks.v1+notebook": {
            "dashboards": [],
            "language": "python",
            "notebookMetadata": {
                "mostRecentlyExecutedCommandWithImplicitDF": {
                    "commandId": -1,
                    "dataframes": [
                        "_sqldf"
                    ]
                },
                "pythonIndentUnit": 2
            },
            "notebookName": "SPARK DataFrame",
            "widgets": {}
        },
        "colab": {
            "provenance": []
        },
        "language_info": {
            "name": "python"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
